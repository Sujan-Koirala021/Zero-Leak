{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QkdsR-jU32IW",
        "outputId": "22b50cf6-1df9-44a9-aa6d-fb4257fa429e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.62 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.66)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.62->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "7fa17070d9414f78a2b14b0b5541a0f8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3g4s6wVOtgrH",
        "outputId": "23841cda-98dc-4615-815b-f1b039d46a20",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: langgraph-checkpoint>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.1.0)\n",
            "Requirement already satisfied: langgraph-prebuilt>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.5.1)\n",
            "Requirement already satisfied: langgraph-sdk>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.72)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (4.14.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint>=2.1.0->langgraph) (1.10.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk>=0.1.42->langgraph) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk>=0.1.42->langgraph) (1.3.1)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.66)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain_community) (2.33.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain_community) (1.3.1)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain_community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain_community-0.3.26 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langgraph\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "token = userdata.get('github_pat')"
      ],
      "metadata": {
        "id": "fmDAoGw1yO3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b31ab40d",
        "outputId": "891ee61e-228c-4dc8-a246-4f4db2f49b38"
      },
      "source": [
        "import os\n",
        "gemini_api_key = userdata.get('Gemini_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = gemini_api_key\n",
        "print(\"Google API key configured.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google API key configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f07eb889",
        "outputId": "46801aed-cd05-4777-994c-c301481667ea"
      },
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
        "\n",
        "print(\"Gemini LLM initialized.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemini LLM initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "198ab8d7"
      },
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of the LangGraph for the secret detection workflow.\n",
        "\n",
        "    Attributes:\n",
        "        repo_url: The URL of the GitHub repository being analyzed.\n",
        "        loaded_documents: A list of Document objects representing the loaded files.\n",
        "        potential_secrets: A list of dictionaries, each containing details about a potential secret found by pattern matching.\n",
        "        merged_context_ranges: A dictionary where keys are file paths and values are merged context ranges (list of tuples).\n",
        "        llm_analysis_results: A dictionary where keys are file paths and values are the LLM's analysis of the context.\n",
        "    \"\"\"\n",
        "    repo_url: str\n",
        "    loaded_documents: List[Document]\n",
        "    potential_secrets: List[dict]\n",
        "    merged_context_ranges: dict\n",
        "    llm_analysis_results: dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96f24931"
      },
      "source": [
        "from urllib.parse import urlparse\n",
        "import os\n",
        "from langchain_community.document_loaders import GithubFileLoader\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def create_github_file_loader(repo_url):\n",
        "    \"\"\"\n",
        "    Creates a GithubFileLoader for the given repository URL, excluding binary files.\n",
        "    \"\"\"\n",
        "    parsed_url = urlparse(repo_url)\n",
        "    path_parts = parsed_url.path.strip('/').split('/')\n",
        "    if len(path_parts) < 2:\n",
        "        raise ValueError(f\"Invalid GitHub repository URL: {repo_url}\")\n",
        "    repo_owner = path_parts[0]\n",
        "    repo_name = path_parts[1]\n",
        "\n",
        "    # Define a list of common binary file extensions to exclude\n",
        "    binary_extensions = [\n",
        "    # Images\n",
        "    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.tif', '.raw', '.heic', '.webp', '.svgz', '.ico', '.dds', '.psd',\n",
        "    # Videos\n",
        "    '.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.mpeg', '.mpg', '.3gp', '.m4v', '.vob',\n",
        "    # Audio\n",
        "    '.mp3', '.wav', '.flac', '.aac', '.ogg', '.wma', '.m4a', '.alac', '.opus',\n",
        "    # Archives and Compressed Files\n",
        "    '.zip', '.tar', '.gz', '.rar', '.7z', '.bz2', '.xz', '.lz', '.lzma', '.cab', '.iso', '.dmg', '.arj', '.z',\n",
        "    # Documents (often binary)\n",
        "    '.pdf', '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.odt', '.ods', '.odp', '.pages', '.key', '.numbers',\n",
        "    # Databases\n",
        "    '.sqlite', '.db', '.mdb', '.accdb', '.dbf', '.sql', '.ndb',\n",
        "    # Executables and Libraries\n",
        "    '.exe', '.dll', '.so', '.dylib', '.bin', '.app', '.deb', '.rpm', '.apk', '.bat', '.com', '.jar', '.class', '.o', '.a', '.lib', '.sys',\n",
        "    # Fonts\n",
        "    '.woff', '.woff2', '.ttf', '.otf', '.eot', '.fon', '.fnt',\n",
        "    # Disk and System Images\n",
        "    '.iso', '.img', '.vmdk', '.vdi', '.vhd', '.vhdx', '.qcow2', '.raw',\n",
        "    # Miscellaneous Binary Formats\n",
        "    '.torrent', '.bak', '.lock', '.dat', '.bin', '.lockb'\n",
        "]\n",
        "\n",
        "    # Assuming 'token' is a global variable containing the GitHub Personal Access Token\n",
        "    if 'token' not in globals() or not globals()['token']:\n",
        "         raise ValueError(\"GitHub Personal Access Token not found. Please ensure the 'token' variable is set.\")\n",
        "\n",
        "    return GithubFileLoader(\n",
        "        repo=f\"{repo_owner}/{repo_name}\",\n",
        "        access_token=token,\n",
        "        # Use a lambda function to filter out files with binary extensions\n",
        "        file_filter=lambda file_path: not any(file_path.lower().endswith(ext) for ext in binary_extensions),\n",
        "    )\n",
        "\n",
        "\n",
        "def load_files_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Loads files from the GitHub repository specified in the state.\n",
        "    \"\"\"\n",
        "    print(\"---LOADING FILES---\")\n",
        "    repo_url = state['repo_url']\n",
        "    loaded_documents = []\n",
        "\n",
        "    try:\n",
        "        loader = create_github_file_loader(repo_url)\n",
        "        all_file_paths = loader.get_file_paths()\n",
        "\n",
        "        for file_info in all_file_paths:\n",
        "            file_path = file_info['path']\n",
        "            try:\n",
        "                document_content = loader.get_file_content_by_path(file_path)\n",
        "                loaded_documents.append(Document(page_content=document_content, metadata={\"file_path\": file_path}))\n",
        "            except UnicodeDecodeError:\n",
        "                print(f\"Skipping file due to decoding error: {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while loading {file_path}: {e}\")\n",
        "\n",
        "        print(f\"Successfully loaded {len(loaded_documents)} files.\")\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"Error creating GitHub file loader: {ve}\")\n",
        "        # Handle the error, perhaps by updating state to indicate failure\n",
        "        # For now, we'll just pass and the loaded_documents list will be empty\n",
        "        pass\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during file loading: {e}\")\n",
        "        pass\n",
        "\n",
        "\n",
        "    return {**state, 'loaded_documents': loaded_documents}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36ba488d"
      },
      "source": [
        "import re\n",
        "\n",
        "def pattern_matching_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Performs pattern matching on loaded documents to find potential secrets.\n",
        "    \"\"\"\n",
        "    print(\"---PERFORMING PATTERN MATCHING---\")\n",
        "    loaded_documents = state.get('loaded_documents', [])\n",
        "    potential_secrets = []\n",
        "\n",
        "\n",
        "    secret_patterns = [\n",
        "        r'AKIA[0-9A-Z]{16}',  # AWS Access Key ID\n",
        "        r'SKIA[0-9A-Z]{16}',  # AWS Secret Access Key\n",
        "        r'AIza[0-9A-Za-z-_]{35}',  # Google API Key\n",
        "        r'ya29\\.[0-9A-Za-z\\-_]+',  # Google OAuth Access Token\n",
        "        r'ghp_[0-9a-zA-Z]{36}',  # GitHub Personal Access Token\n",
        "        r'gho_[0-9a-zA-Z]{36}',  # GitHub OAuth Token\n",
        "        r'ghu_[0-9a-zA-Z]{36}',  # GitHub User-to-Server Token\n",
        "        r'ghs_[0-9a-zA-Z]{36}',  # GitHub Server-to-Server Token\n",
        "        r'sk-([a-zA-Z0-9]{32}|[a-zA-Z0-9]{48})',  # OpenAI API Key\n",
        "        r'pk-([a-zA-Z0-9]{32}|[a-zA-Z0-9]{48})',  # OpenAI Publishable Key\n",
        "        r'Bearer [A-Za-z0-9\\-\\._~+\\/]+=*',  # Generic Bearer Token\n",
        "        r'password\\s*=\\s*[\\'\"]([^\\'\"]+)[\\'\"]',  # Password in assignment\n",
        "        r'secret\\s*=\\s*[\\'\"]([^\\'\"]+)[\\'\"]',  # Secret in assignment\n",
        "        r'api_key\\s*=\\s*[\\'\"]([^\\'\"]+)[\\'\"]',  # API Key in assignment\n",
        "        r'auth_token\\s*=\\s*[\\'\"]([^\\'\"]+)[\\'\"]',  # Auth Token in assignment\n",
        "        r'-----BEGIN PRIVATE KEY-----.*-----END PRIVATE KEY-----', # PEM private key\n",
        "        r'ssh-rsa\\s+[A-Za-z0-9+\\/]+={0,2}(\\s+[^\\s]+)?', # SSH private key\n",
        "        r'ssh-ed25519\\s+[A-Za-z0-9+\\/]+={0,2}(\\s+[^\\s]+)?', # SSH private key\n",
        "        r'PGP PRIVATE KEY BLOCK', # PGP private key\n",
        "        r'client_secret\\s*=\\s*[\\'\"]([^\\'\"]+)[\\'\"]', # Client secret\n",
        "        r'client_id\\s*=\\s*[\\'\"]([^\\'\"]+)[\\'\"]', # Client ID (less sensitive but often paired)\n",
        "        r'(\\w+)?key(\\w+)?', # Keyword 'key'\n",
        "        r'(\\w+)?secret(\\w+)?', # Keyword 'secret'\n",
        "        r'(\\w+)?password(\\w+)?', # Keyword 'password'\n",
        "        r'(\\w+)?token(\\w+)?', # Keyword 'token'\n",
        "        r'(\\w+)?credential(\\w+)?', # Keyword 'credential'\n",
        "    ]\n",
        "\n",
        "\n",
        "    for document in loaded_documents:\n",
        "        for pattern in secret_patterns:\n",
        "            for match in re.finditer(pattern, document.page_content):\n",
        "                 potential_secrets.append({\n",
        "                    \"file_path\": document.metadata.get('file_path', 'N/A'),\n",
        "                    \"pattern\": pattern,\n",
        "                    \"match_str\": match.group(0),\n",
        "                    \"start_index\": match.start(),\n",
        "                    \"end_index\": match.end()\n",
        "                })\n",
        "\n",
        "    print(f\"Found {len(potential_secrets)} potential secret occurrences.\")\n",
        "\n",
        "    return {**state, 'potential_secrets': potential_secrets}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e84afaa9"
      },
      "source": [
        "def merge_context_ranges_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Merges overlapping context ranges for potential secrets within each file.\n",
        "    Includes the get_context_lines helper function.\n",
        "    \"\"\"\n",
        "    print(\"---MERGING CONTEXT RANGES---\")\n",
        "    potential_secrets = state.get('potential_secrets', [])\n",
        "    loaded_documents = state.get('loaded_documents', [])\n",
        "\n",
        "\n",
        "    def get_context_lines(content, match_start, match_end, lines_before=3, lines_after=3):\n",
        "        \"\"\"\n",
        "        Extracts the start and end line numbers around a matched section in a string.\n",
        "        \"\"\"\n",
        "        lines = content.splitlines()\n",
        "        start_char_index = 0\n",
        "        start_line_index = -1\n",
        "        end_line_index = -1\n",
        "\n",
        "        for i, line in enumerate(lines):\n",
        "            end_char_index = start_char_index + len(line) + 1\n",
        "\n",
        "            if start_char_index <= match_start < end_char_index or \\\n",
        "               start_char_index <= match_end < end_char_index or \\\n",
        "               (match_start < start_char_index and match_end >= start_char_index):\n",
        "\n",
        "                if start_line_index == -1:\n",
        "                     start_line_index = max(0, i - lines_before)\n",
        "                end_line_index = min(len(lines), i + lines_after + 1)\n",
        "\n",
        "            start_char_index = end_char_index\n",
        "\n",
        "        if start_line_index == -1:\n",
        "            return (-1, -1)\n",
        "\n",
        "        return (start_line_index, end_line_index)\n",
        "\n",
        "    file_context_ranges = {}\n",
        "\n",
        "    for finding in potential_secrets:\n",
        "        file_path = finding['file_path']\n",
        "        document_content = None\n",
        "        for doc in loaded_documents:\n",
        "            if doc.metadata.get('file_path') == file_path:\n",
        "                document_content = doc.page_content\n",
        "                break\n",
        "\n",
        "        if document_content:\n",
        "            start_line, end_line = get_context_lines(document_content, finding['start_index'], finding['end_index'])\n",
        "\n",
        "            if (start_line, end_line) != (-1, -1):\n",
        "                if file_path not in file_context_ranges:\n",
        "                    file_context_ranges[file_path] = []\n",
        "                file_context_ranges[file_path].append((start_line, end_line))\n",
        "\n",
        "    merged_file_context_ranges = {}\n",
        "\n",
        "    for file_path, ranges in file_context_ranges.items():\n",
        "        sorted_ranges = sorted(ranges)\n",
        "        merged_ranges = []\n",
        "        if sorted_ranges:\n",
        "            current_merged_range = list(sorted_ranges[0])\n",
        "\n",
        "            for start, end in sorted_ranges[1:]:\n",
        "                if start <= current_merged_range[1]:\n",
        "                    current_merged_range[1] = max(current_merged_range[1], end)\n",
        "                else:\n",
        "                    merged_ranges.append(tuple(current_merged_range))\n",
        "                    current_merged_range = [start, end]\n",
        "            merged_ranges.append(tuple(current_merged_range))\n",
        "        merged_file_context_ranges[file_path] = merged_ranges\n",
        "\n",
        "    print(\"Merged context ranges calculated.\")\n",
        "\n",
        "    return {**state, 'merged_context_ranges': merged_file_context_ranges}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "503d4a82"
      },
      "source": [
        "def analyze_with_llm_node(state: GraphState) -> GraphState:\n",
        "    \"\"\"\n",
        "    Analyzes extracted contexts with the LLM to verify the presence of secrets.\n",
        "    \"\"\"\n",
        "    print(\"---ANALYZING WITH LLM---\")\n",
        "    merged_context_ranges = state.get('merged_context_ranges', {})\n",
        "    loaded_documents = state.get('loaded_documents', [])\n",
        "    llm_analysis_results = {}\n",
        "\n",
        "    # Ensure llm is accessible in this scope (assuming it's a global variable or passed in)\n",
        "    # If not accessible, this will raise a NameError.\n",
        "    if 'llm' not in globals():\n",
        "         raise ValueError(\"LLM is not initialized. Please ensure the 'llm' variable is set.\")\n",
        "\n",
        "\n",
        "    for file_path, merged_ranges in merged_context_ranges.items():\n",
        "        document_content = None\n",
        "        for doc in loaded_documents:\n",
        "            if doc.metadata.get('file_path') == file_path:\n",
        "                document_content = doc.page_content\n",
        "                break\n",
        "\n",
        "        if document_content:\n",
        "            lines = document_content.splitlines()\n",
        "            file_context_text = \"\"\n",
        "            for start_line, end_line in merged_ranges:\n",
        "                context_text = \"\\n\".join(lines[start_line:end_line])\n",
        "                file_context_text += f\"--- Context from lines {start_line+1}-{end_line} ---\\n\"\n",
        "                file_context_text += context_text + \"\\n\\n\"\n",
        "\n",
        "            prompt = f\"\"\"Analyze the following text extracted from a file.\n",
        "This text contains several sections from the file that were flagged as potentially containing secrets based on pattern matching.\n",
        "Review the provided context and determine if any of these sections genuinely contain secrets such as API keys, passwords, private keys, or sensitive credentials.\n",
        "For each potential genuine secret you find:\n",
        "- Specify the file path.\n",
        "- Point to the specific line numbers or provide the relevant snippet from the context.\n",
        "- Explain why you believe it is a secret.\n",
        "If you do not find any genuine secrets in the provided context, state that clearly.\n",
        "\n",
        "File Path: {file_path}\n",
        "\n",
        "Context:\n",
        "{file_context_text}\n",
        "\"\"\"\n",
        "            try:\n",
        "                llm_response = llm.invoke(prompt)\n",
        "                llm_analysis_results[file_path] = llm_response.content\n",
        "            except Exception as e:\n",
        "                llm_analysis_results[file_path] = f\"Error during LLM analysis: {e}\"\n",
        "        else:\n",
        "             llm_analysis_results[file_path] = \"Could not retrieve content for analysis.\"\n",
        "\n",
        "\n",
        "    print(\"LLM analysis complete.\")\n",
        "    return {**state, 'llm_analysis_results': llm_analysis_results}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "566f0b85"
      },
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# Instantiate the StateGraph with the defined GraphState\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Add nodes to the graph\n",
        "workflow.add_node(\"load_files\", load_files_node)\n",
        "workflow.add_node(\"pattern_matching\", pattern_matching_node)\n",
        "workflow.add_node(\"merge_context_ranges\", merge_context_ranges_node)\n",
        "workflow.add_node(\"analyze_with_llm\", analyze_with_llm_node)\n",
        "\n",
        "\n",
        "# Set the entry point\n",
        "workflow.set_entry_point(\"load_files\")\n",
        "\n",
        "# Add edges for the linear workflow\n",
        "workflow.add_edge(\"load_files\", \"pattern_matching\")\n",
        "workflow.add_edge(\"pattern_matching\", \"merge_context_ranges\")\n",
        "workflow.add_edge(\"merge_context_ranges\", \"analyze_with_llm\")\n",
        "\n",
        "# Set the finish point\n",
        "workflow.set_finish_point(\"analyze_with_llm\")\n",
        "\n",
        "# Compile the graph\n",
        "app = workflow.compile()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVYklcZ5OtTv",
        "outputId": "8af2e68f-ddd7-4877-bc1c-b3e62382cf85"
      },
      "source": [
        "import re\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Re-compile the graph with the updated node definition\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"load_files\", load_files_node)\n",
        "workflow.add_node(\"pattern_matching\", pattern_matching_node)\n",
        "# Add the updated node definition\n",
        "workflow.add_node(\"merge_context_ranges\", merge_context_ranges_node)\n",
        "workflow.add_node(\"analyze_with_llm\", analyze_with_llm_node)\n",
        "\n",
        "workflow.set_entry_point(\"load_files\")\n",
        "workflow.add_edge(\"load_files\", \"pattern_matching\")\n",
        "workflow.add_edge(\"pattern_matching\", \"merge_context_ranges\")\n",
        "workflow.add_edge(\"merge_context_ranges\", \"analyze_with_llm\")\n",
        "workflow.set_finish_point(\"analyze_with_llm\")\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"LangGraph workflow redefined and compiled with updated merge_context_ranges node.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph workflow redefined and compiled with updated merge_context_ranges node.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b7c5035",
        "outputId": "019eaaab-d331-4e81-a28b-8f6964b9c217"
      },
      "source": [
        "\n",
        "\n",
        "initial_state = {\n",
        "    \"repo_url\": \"https://github.com/SusheelThapa/gastro\", # Replace with the actual repository URL\n",
        "    \"loaded_documents\": [],\n",
        "    \"potential_secrets\": [],\n",
        "    \"merged_context_ranges\": {},\n",
        "    \"llm_analysis_results\": {}\n",
        "}\n",
        "\n",
        "print(\"Invoking the LangGraph application again with the updated workflow...\")\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "print(\"\\nLangGraph execution complete.\")\n",
        "print(\"Final State:\")\n",
        "# Optionally print some parts of the final state to verify\n",
        "print(f\"  Number of loaded documents: {len(final_state.get('loaded_documents', []))}\")\n",
        "print(f\"  Number of potential secret occurrences: {len(final_state.get('potential_secrets', []))}\")\n",
        "print(f\"  Files with merged context ranges: {list(final_state.get('merged_context_ranges', {}).keys())}\")\n",
        "print(f\"  Files with LLM analysis results: {list(final_state.get('llm_analysis_results', {}).keys())}\")\n",
        "\n",
        "# Print the LLM analysis results\n",
        "print(\"\\n--- LLM Analysis Results ---\")\n",
        "for file_path, analysis in final_state.get('llm_analysis_results', {}).items():\n",
        "    print(f\"File: {file_path}\")\n",
        "    print(f\"Analysis:\\n{analysis}\")\n",
        "    print(\"-\" * 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invoking the LangGraph application again with the updated workflow...\n",
            "---LOADING FILES---\n",
            "Successfully loaded 67 files.\n",
            "---PERFORMING PATTERN MATCHING---\n",
            "Found 55 potential secret occurrences.\n",
            "---MERGING CONTEXT RANGES---\n",
            "Merged context ranges calculated.\n",
            "---ANALYZING WITH LLM---\n",
            "LLM analysis complete.\n",
            "\n",
            "LangGraph execution complete.\n",
            "Final State:\n",
            "  Number of loaded documents: 67\n",
            "  Number of potential secret occurrences: 55\n",
            "  Files with merged context ranges: ['README.md', 'agent/recipe/agent.py', 'agent/requirements.txt', 'gastro/components/gastro-answer.tsx', 'gastro/components/gastro-search.tsx', 'gastro/components/gastro-wrapper.tsx', 'gastro/components/recipe-card.tsx', 'gastro/components/recipe-modal.tsx', 'gastro/package.json', 'notebook/Gastro-Langgraph.ipynb']\n",
            "  Files with LLM analysis results: ['README.md', 'agent/recipe/agent.py', 'agent/requirements.txt', 'gastro/components/gastro-answer.tsx', 'gastro/components/gastro-search.tsx', 'gastro/components/gastro-wrapper.tsx', 'gastro/components/recipe-card.tsx', 'gastro/components/recipe-modal.tsx', 'gastro/package.json', 'notebook/Gastro-Langgraph.ipynb']\n",
            "\n",
            "--- LLM Analysis Results ---\n",
            "File: README.md\n",
            "Analysis:\n",
            "Upon reviewing the provided context from `README.md`, I have analyzed the flagged sections for potential secrets.\n",
            "\n",
            "Based on the analysis, I conclude that **no genuine secrets** are present in the provided text.\n",
            "\n",
            "Here's a breakdown of the findings:\n",
            "\n",
            "*   **File Path:** `README.md`\n",
            "\n",
            "*   **Context from lines 91-97:**\n",
            "    ```bash\n",
            "    OPENAI_API_KEY=your-nagai-api-key\n",
            "    OPENAI_BASE_URL=https://api.naga.ac/v1\n",
            "    REMOTE_ACTION_BASE_URL=http://127.0.0.1:8000\n",
            "    ```\n",
            "    *   `OPENAI_API_KEY=your-nagai-api-key`: This is a placeholder value. The text \"your-nagai-api-key\" explicitly indicates that a real key needs to be inserted here by the user. It is not a genuine secret.\n",
            "    *   `OPENAI_BASE_URL` and `REMOTE_ACTION_BASE_URL` are URLs, not secrets.\n",
            "\n",
            "*   **Context from lines 100-114:**\n",
            "    ```\n",
            "    PORT=8000\n",
            "    OPENAI_API_KEY=your-nagaai-api-key\n",
            "    OPENAI_BASE_URL=https://api.naga.ac/v1/\n",
            "    SPOONACULAR_API_KEY=your-spoonacular-api-key\n",
            "    APP_ENV=development\n",
            "    ```\n",
            "    *   `PORT=8000` and `APP_ENV=development` are configuration settings, not secrets.\n",
            "    *   `OPENAI_API_KEY=your-nagaai-api-key`: Again, this is a placeholder value, clearly indicating that a real key should be inserted. It is not a genuine secret.\n",
            "    *   `OPENAI_BASE_URL` is a URL, not a secret.\n",
            "    *   `SPOONACULAR_API_KEY=your-spoonacular-api-key`: This is also a placeholder value. The text \"your-spoonacular-api-key\" indicates that a real key needs to be inserted. It is not a genuine secret.\n",
            "\n",
            "All instances that appear to be API keys are clearly marked as placeholders (e.g., \"your-nagai-api-key\", \"your-spoonacular-api-key\"), indicating they are examples for configuration rather than actual sensitive credentials.\n",
            "--------------------\n",
            "File: agent/recipe/agent.py\n",
            "Analysis:\n",
            "Based on the review of the provided context, I did not find any genuine secrets hardcoded in the `agent/recipe/agent.py` file.\n",
            "\n",
            "All instances where API keys are mentioned (`OPENAI_API_KEY`, `SPOONACULAR_API_KEY`) are loaded from environment variables using `os.getenv()`. This is a standard and secure practice for managing sensitive credentials, preventing them from being directly exposed in the source code.\n",
            "\n",
            "For example:\n",
            "*   **Lines 60-66, 132-139, 218-225:** `openai_api_key=os.getenv(\"OPENAI_API_KEY\")`\n",
            "*   **Lines 205-215, 271-277:** `spoonacular_api_key = os.getenv(\"SPOONACULAR_API_KEY\")`\n",
            "\n",
            "While these lines deal with API keys, the keys themselves are not present in the code; rather, the code instructs the system to retrieve them from a secure external source (environment variables). Therefore, no genuine secrets are directly exposed in this file.\n",
            "--------------------\n",
            "File: agent/requirements.txt\n",
            "Analysis:\n",
            "Upon reviewing the provided text from `agent/requirements.txt`, specifically lines 58-64:\n",
            "\n",
            "```\n",
            "--- Context from lines 58-64 ---\n",
            "SQLAlchemy==2.0.36\n",
            "starlette==0.41.3\n",
            "tenacity==9.0.0\n",
            "tiktoken==0.8.0\n",
            "toml==0.10.2\n",
            "tqdm==4.67.1\n",
            "typing-inspect==0.9.0\n",
            "```\n",
            "\n",
            "This section lists Python package dependencies and their exact versions, which is typical content for a `requirements.txt` file.\n",
            "\n",
            "I do not find any genuine secrets such as API keys, passwords, private keys, or sensitive credentials in this context. The listed items are all public library names and their version numbers.\n",
            "--------------------\n",
            "File: gastro/components/gastro-answer.tsx\n",
            "Analysis:\n",
            "Based on the provided context, I do not find any genuine secrets.\n",
            "\n",
            "The text extract shows standard React/TypeScript JSX code for rendering UI components (`div`, `RecipeCard`). The attributes and values (`key`, `id`, `handleShowMore`, `title`, `recipe.id`, `recipe.title`) are all related to component props and data mapping, not sensitive credentials or keys.\n",
            "--------------------\n",
            "File: gastro/components/gastro-search.tsx\n",
            "Analysis:\n",
            "Based on the provided context, I have reviewed the flagged sections from the file `gastro/components/gastro-search.tsx`.\n",
            "\n",
            "I do not find any genuine secrets such as API keys, passwords, private keys, or sensitive credentials in the provided snippets.\n",
            "\n",
            "The code snippets show standard React/TypeScript component logic for:\n",
            "*   **Lines 57-63**: Handling user input, state management (`value={gastroInput}`, `onChange`, `onKeyDown`), and triggering a function (`handleGastro`).\n",
            "*   **Lines 68-74**: Rendering UI components (`CategoryCard`) by mapping over a `suggestions` array and passing props (`key`, `icon`, `title`, `handleGastro`).\n",
            "\n",
            "None of these lines contain hardcoded strings or patterns that resemble sensitive credentials. The terms like `key` and `value` are used in their common programming context (e.g., `e.key` for keyboard events, `key={suggestion.label}` for React list keys) and do not indicate the presence of a secret.\n",
            "--------------------\n",
            "File: gastro/components/gastro-wrapper.tsx\n",
            "Analysis:\n",
            "Based on the provided context from `gastro/components/gastro-wrapper.tsx` (lines 11-27), I do not find any genuine secrets such as API keys, passwords, private keys, or sensitive credentials.\n",
            "\n",
            "The provided code snippet is a JSX structure for a React component, handling conditional rendering and animations. It contains no hardcoded sensitive values or patterns indicative of secrets.\n",
            "--------------------\n",
            "File: gastro/components/recipe-card.tsx\n",
            "Analysis:\n",
            "Based on the provided context from the file `gastro/components/recipe-card.tsx`, I do not find any genuine secrets such as API keys, passwords, private keys, or sensitive credentials.\n",
            "\n",
            "The provided snippet (lines 65-71) is standard JSX/TypeScript code for rendering a list of ingredients in a UI component. It contains no patterns or values that indicate the presence of sensitive information.\n",
            "--------------------\n",
            "File: gastro/components/recipe-modal.tsx\n",
            "Analysis:\n",
            "Based on the provided context from `gastro/components/recipe-modal.tsx`, I do not find any genuine secrets such as API keys, passwords, private keys, or sensitive credentials.\n",
            "\n",
            "The text snippet from lines 34-40 appears to be standard UI code (JSX/TSX) for rendering a list of ingredients within a recipe modal. It contains HTML elements, CSS class names, and React mapping logic, but no information that constitutes a secret.\n",
            "--------------------\n",
            "File: gastro/package.json\n",
            "Analysis:\n",
            "Upon reviewing the provided context from `gastro/package.json`, lines 27-33, I can confirm that **no genuine secrets** are present.\n",
            "\n",
            "The context displays a standard section of a `package.json` file, listing development dependencies with their respective version numbers. For example:\n",
            "\n",
            "*   `\"openai\": \"^4.77.0\"`\n",
            "*   `\"react\": \"^19.0.0\"`\n",
            "*   `\"react-dom\": \"^19.0.0\"`\n",
            "*   `\"react-hotkeys-hook\": \"^4.6.1\"`\n",
            "*   `\"react-markdown\": \"^9.0.1\"`\n",
            "*   `\"tailwind-merge\": \"^2.6.0\"`\n",
            "*   `\"tailwindcss-animate\": \"^1.0.7\"`\n",
            "\n",
            "These entries simply indicate the names of software packages and the compatible versions required for the project. They do not contain any sensitive information such as API keys, passwords, private keys, or other credentials. Secrets are typically found in configuration files, environment variables, or sometimes directly embedded in source code (though this is a bad practice), not within the dependency list of a `package.json` file.\n",
            "--------------------\n",
            "File: notebook/Gastro-Langgraph.ipynb\n",
            "Analysis:\n",
            "Based on the provided context, no genuine secrets such as actual API keys, passwords, private keys, or sensitive credentials are found.\n",
            "\n",
            "The text mentions `openai_api_key` and `spoonacular_api_key` in the following ways:\n",
            "\n",
            "1.  **Class Initialization:**\n",
            "    *   **File Path:** `notebook/Gastro-Langgraph.ipynb`\n",
            "    *   **Snippet:**\n",
            "        ```python\n",
            "        class LangraphAgent:\n",
            "            def __init__(self, openai_api_key, spoonacular_api_key, custom_base_url):\n",
            "                self.openai_api_key = openai_api_key\n",
            "                self.spoonacular_api_key = spoonacular_api_key\n",
            "                self.model = ChatOpenAI(\n",
            "                    model=\"gpt-4o-mini\",\n",
            "                    temperature=0,\n",
            "                    openai_api_key=openai_api_key,\n",
            "                    base_url=custom_base_url,\n",
            "                )\n",
            "        ```\n",
            "    *   **Explanation:** This code defines a class constructor that *accepts* API keys as parameters. This is a standard and secure way to handle credentials, as the actual values are expected to be passed in at runtime rather than being hardcoded within the class definition itself.\n",
            "\n",
            "2.  **API Call Usage:**\n",
            "    *   **File Path:** `notebook/Gastro-Langgraph.ipynb`\n",
            "    *   **Snippet:**\n",
            "        ```python\n",
            "        url = f\"https://api.spoonacular.com/recipes/findByIngredients?ingredients={','.join(ingredients)}&apiKey={self.spoonacular_api_key}\"\n",
            "        response = requests.get(url)\n",
            "        ```\n",
            "    *   **Explanation:** Here, `self.spoonacular_api_key` is used in an API call. This refers to the instance variable that was set during the class initialization (as seen above). The key itself is not hardcoded in this snippet.\n",
            "\n",
            "3.  **Placeholder Values in Usage Example:**\n",
            "    *   **File Path:** `notebook/Gastro-Langgraph.ipynb`\n",
            "    *   **Line Numbers:** 207-208\n",
            "    *   **Snippet:**\n",
            "        ```python\n",
            "        openai_api_key = 'YOUR_OPENAI_API_KEY'  # Replace with your actual API key\n",
            "        spoonacular_api_key = 'YOUR_SPPONACULAR_API_KEY'  # Replace with your actual Spoonacular API key\n",
            "        ```\n",
            "    *   **Explanation:** These lines explicitly show placeholder strings (`'YOUR_OPENAI_API_KEY'`, `'YOUR_SPPONACULAR_API_KEY'`) and comments instructing the user to replace them with actual keys. While these indicate locations where secrets *would* be placed, the values themselves are not genuine secrets. They are generic placeholders intended for demonstration purposes.\n",
            "\n",
            "All other provided context sections contain code logic or API response data, none of which include sensitive credentials.\n",
            "\n",
            "**Conclusion:** No genuine secrets were found in the provided context. The API key references are either part of a constructor definition, usage of an instance variable, or are clearly marked placeholders.\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55689928",
        "outputId": "692ce7a8-80ed-4aea-fa4e-685cf8cc1ef5"
      },
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "\n",
        "repo_url = \"https://github.com/SusheelThapa/gastro\"\n",
        "\n",
        "try:\n",
        "    loader = create_github_file_loader(repo_url)\n",
        "except ValueError as ve:\n",
        "    print(f\"Error creating GitHub file loader: {ve}\")\n",
        "    loader = None\n",
        "\n",
        "loaded_documents = []\n",
        "all_splits = []\n",
        "\n",
        "if loader:\n",
        "\n",
        "    try:\n",
        "        all_file_paths = loader.get_file_paths()\n",
        "\n",
        "\n",
        "        for file_info in all_file_paths:\n",
        "            file_path = file_info['path']\n",
        "            try:\n",
        "\n",
        "                document_content = loader.get_file_content_by_path(file_path)\n",
        "\n",
        "                loaded_documents.append(Document(page_content=document_content, metadata={\"file_path\": file_path}))\n",
        "            except UnicodeDecodeError:\n",
        "                print(f\"Skipping file due to decoding error: {file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while loading {file_path}: {e}\")\n",
        "\n",
        "        print(f\"Successfully loaded {len(loaded_documents)} files.\")\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "                    chunk_size=500, chunk_overlap=200\n",
        "                )\n",
        "\n",
        "\n",
        "        all_splits = text_splitter.split_documents(loaded_documents)\n",
        "\n",
        "\n",
        "\n",
        "        print(f\"Successfully split loaded documents into {len(all_splits)} chunks.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during file loading or splitting: {e}\")\n",
        "else:\n",
        "    print(\"File loader was not initialized due to an error.\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded 67 files.\n",
            "Successfully split loaded documents into 420 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54b1e76b",
        "outputId": "c66c1468-1e2f-4bc7-ce82-e53288368219"
      },
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "\n",
        "if 'all_splits' in globals() and all_splits:\n",
        "    print(f\"Generating embeddings for {len(all_splits)} chunks...\")\n",
        "\n",
        "    try:\n",
        "\n",
        "        texts = [doc.page_content for doc in all_splits]\n",
        "        document_embeddings = embeddings.embed_documents(texts)\n",
        "        print(\"Embeddings generated successfully.\")\n",
        "        chunks_with_embeddings = []\n",
        "        for i, chunk in enumerate(all_splits):\n",
        "            chunks_with_embeddings.append({\n",
        "                \"chunk\": chunk,\n",
        "                \"embedding\": document_embeddings[i]\n",
        "            })\n",
        "        print(f\"Stored chunks with embeddings for {len(chunks_with_embeddings)} chunks.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during embedding generation: {e}\")\n",
        "        document_embeddings = [] # Ensure variable is defined even on error\n",
        "        chunks_with_embeddings = [] # Ensure variable is defined even on error\n",
        "else:\n",
        "    print(\"No document chunks found to generate embeddings.\")\n",
        "    document_embeddings = [] # Ensure variable is defined even on error\n",
        "    chunks_with_embeddings = [] # Ensure variable is defined even on error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings for 420 chunks...\n",
            "Embeddings generated successfully.\n",
            "Stored chunks with embeddings for 420 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f572aac",
        "outputId": "5f464abf-9480-4981-bb3e-31f79a89be92",
        "collapsed": true
      },
      "source": [
        "!pip install pymongo langchain_mongodb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
            "Collecting langchain_mongodb\n",
            "  Downloading langchain_mongodb-0.6.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: langchain-core>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (0.3.8)\n",
            "Requirement already satisfied: langchain>=0.3 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (0.3.26)\n",
            "Collecting lark<2.0.0,>=1.1.9 (from langchain_mongodb)\n",
            "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: numpy>=1.26 in /usr/local/lib/python3.11/dist-packages (from langchain_mongodb) (2.0.2)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.3->langchain_mongodb) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3->langchain_mongodb) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3->langchain_mongodb) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3->langchain_mongodb) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core>=0.3->langchain_mongodb) (4.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core>=0.3->langchain_mongodb) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.3->langchain_mongodb) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.3->langchain_mongodb) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.3->langchain_mongodb) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.3->langchain_mongodb) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.3->langchain_mongodb) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.3->langchain_mongodb) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.3->langchain_mongodb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.3->langchain_mongodb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain>=0.3->langchain_mongodb) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.3->langchain_mongodb) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.3->langchain_mongodb) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.3->langchain_mongodb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.3->langchain_mongodb) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.3->langchain_mongodb) (1.3.1)\n",
            "Downloading pymongo-4.13.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_mongodb-0.6.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lark, dnspython, pymongo, langchain_mongodb\n",
            "Successfully installed dnspython-2.7.0 langchain_mongodb-0.6.2 lark-1.2.2 pymongo-4.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6510807",
        "outputId": "4d1a187b-41e6-4816-b4a0-e4aa52459228"
      },
      "source": [
        "import pymongo\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from google.colab import userdata\n",
        "\n",
        "# Retrieve MongoDB URI from userdata\n",
        "mongo_uri = userdata.get(\"MongoDB_URI\")\n",
        "\n",
        "if not mongo_uri:\n",
        "    raise ValueError(\"MongoDB_URI not found in userdata.\")\n",
        "\n",
        "# Database and collection names\n",
        "db_name = \"github_repo_security\"\n",
        "collection_name = \"vulnerability_chunks\"\n",
        "collection = pymongo.MongoClient(mongo_uri)[db_name][collection_name]\n",
        "\n",
        "\n",
        "try:\n",
        "    vectorstore = MongoDBAtlasVectorSearch(\n",
        "        collection=collection, embedding=embeddings, index_name=\"vector_index\", relevance_score_fn=\"cosine\",\n",
        "    )\n",
        "    print(\"MongoDB Atlas Vector Search initialized.\")\n",
        "\n",
        "\n",
        "    if 'chunks_with_embeddings' in globals() and chunks_with_embeddings:\n",
        "        print(f\"Adding {len(chunks_with_embeddings)} chunks to the vector database...\")\n",
        "        # Extract documents and embeddings\n",
        "        docs_to_add = [item[\"chunk\"] for item in chunks_with_embeddings]\n",
        "\n",
        "        for i, doc in enumerate(docs_to_add):\n",
        "             doc.metadata['embedding'] = chunks_with_embeddings[i]['embedding']\n",
        "\n",
        "\n",
        "        result = vectorstore.add_documents(docs_to_add)\n",
        "        print(f\"Added documents with IDs: {result}\")\n",
        "        print(\"Document chunks added to MongoDB vector database.\")\n",
        "    else:\n",
        "        print(\"No chunks with embeddings found to add to the vector database.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during MongoDB vector database setup or document insertion: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MongoDB Atlas Vector Search initialized.\n",
            "Adding 420 chunks to the vector database...\n",
            "Added documents with IDs: ['685ec4bc6a98a11244a76dbc', '685ec4bc6a98a11244a76dbd', '685ec4bc6a98a11244a76dbe', '685ec4bc6a98a11244a76dbf', '685ec4bc6a98a11244a76dc0', '685ec4bc6a98a11244a76dc1', '685ec4bc6a98a11244a76dc2', '685ec4bc6a98a11244a76dc3', '685ec4bc6a98a11244a76dc4', '685ec4bc6a98a11244a76dc5', '685ec4bc6a98a11244a76dc6', '685ec4bc6a98a11244a76dc7', '685ec4bc6a98a11244a76dc8', '685ec4bc6a98a11244a76dc9', '685ec4bc6a98a11244a76dca', '685ec4bc6a98a11244a76dcb', '685ec4bc6a98a11244a76dcc', '685ec4bc6a98a11244a76dcd', '685ec4bc6a98a11244a76dce', '685ec4bc6a98a11244a76dcf', '685ec4bc6a98a11244a76dd0', '685ec4bc6a98a11244a76dd1', '685ec4bc6a98a11244a76dd2', '685ec4bc6a98a11244a76dd3', '685ec4bc6a98a11244a76dd4', '685ec4bc6a98a11244a76dd5', '685ec4bc6a98a11244a76dd6', '685ec4bc6a98a11244a76dd7', '685ec4bc6a98a11244a76dd8', '685ec4bc6a98a11244a76dd9', '685ec4bc6a98a11244a76dda', '685ec4bc6a98a11244a76ddb', '685ec4bc6a98a11244a76ddc', '685ec4bc6a98a11244a76ddd', '685ec4bc6a98a11244a76dde', '685ec4bc6a98a11244a76ddf', '685ec4bc6a98a11244a76de0', '685ec4bc6a98a11244a76de1', '685ec4bc6a98a11244a76de2', '685ec4bc6a98a11244a76de3', '685ec4bc6a98a11244a76de4', '685ec4bc6a98a11244a76de5', '685ec4bc6a98a11244a76de6', '685ec4bc6a98a11244a76de7', '685ec4bc6a98a11244a76de8', '685ec4bc6a98a11244a76de9', '685ec4bc6a98a11244a76dea', '685ec4bc6a98a11244a76deb', '685ec4bc6a98a11244a76dec', '685ec4bc6a98a11244a76ded', '685ec4bc6a98a11244a76dee', '685ec4bc6a98a11244a76def', '685ec4bc6a98a11244a76df0', '685ec4bc6a98a11244a76df1', '685ec4bc6a98a11244a76df2', '685ec4bc6a98a11244a76df3', '685ec4bc6a98a11244a76df4', '685ec4bc6a98a11244a76df5', '685ec4bc6a98a11244a76df6', '685ec4bc6a98a11244a76df7', '685ec4bc6a98a11244a76df8', '685ec4bc6a98a11244a76df9', '685ec4bc6a98a11244a76dfa', '685ec4bc6a98a11244a76dfb', '685ec4bc6a98a11244a76dfc', '685ec4bc6a98a11244a76dfd', '685ec4bc6a98a11244a76dfe', '685ec4bc6a98a11244a76dff', '685ec4bc6a98a11244a76e00', '685ec4bc6a98a11244a76e01', '685ec4bc6a98a11244a76e02', '685ec4bc6a98a11244a76e03', '685ec4bc6a98a11244a76e04', '685ec4bc6a98a11244a76e05', '685ec4bc6a98a11244a76e06', '685ec4bc6a98a11244a76e07', '685ec4bc6a98a11244a76e08', '685ec4bc6a98a11244a76e09', '685ec4bc6a98a11244a76e0a', '685ec4bc6a98a11244a76e0b', '685ec4bc6a98a11244a76e0c', '685ec4bc6a98a11244a76e0d', '685ec4bc6a98a11244a76e0e', '685ec4bc6a98a11244a76e0f', '685ec4bc6a98a11244a76e10', '685ec4bc6a98a11244a76e11', '685ec4bc6a98a11244a76e12', '685ec4bc6a98a11244a76e13', '685ec4bc6a98a11244a76e14', '685ec4bc6a98a11244a76e15', '685ec4bc6a98a11244a76e16', '685ec4bc6a98a11244a76e17', '685ec4bc6a98a11244a76e18', '685ec4bc6a98a11244a76e19', '685ec4bc6a98a11244a76e1a', '685ec4bc6a98a11244a76e1b', '685ec4bc6a98a11244a76e1c', '685ec4bc6a98a11244a76e1d', '685ec4bc6a98a11244a76e1e', '685ec4bc6a98a11244a76e1f', '685ec4bc6a98a11244a76e20', '685ec4bc6a98a11244a76e21', '685ec4bc6a98a11244a76e22', '685ec4bc6a98a11244a76e23', '685ec4bc6a98a11244a76e24', '685ec4bc6a98a11244a76e25', '685ec4bc6a98a11244a76e26', '685ec4bc6a98a11244a76e27', '685ec4bc6a98a11244a76e28', '685ec4bc6a98a11244a76e29', '685ec4bc6a98a11244a76e2a', '685ec4bc6a98a11244a76e2b', '685ec4bc6a98a11244a76e2c', '685ec4bc6a98a11244a76e2d', '685ec4bc6a98a11244a76e2e', '685ec4bc6a98a11244a76e2f', '685ec4bc6a98a11244a76e30', '685ec4bc6a98a11244a76e31', '685ec4bc6a98a11244a76e32', '685ec4bc6a98a11244a76e33', '685ec4bc6a98a11244a76e34', '685ec4bc6a98a11244a76e35', '685ec4bc6a98a11244a76e36', '685ec4bc6a98a11244a76e37', '685ec4bc6a98a11244a76e38', '685ec4bc6a98a11244a76e39', '685ec4bc6a98a11244a76e3a', '685ec4bc6a98a11244a76e3b', '685ec4bc6a98a11244a76e3c', '685ec4bc6a98a11244a76e3d', '685ec4bc6a98a11244a76e3e', '685ec4bc6a98a11244a76e3f', '685ec4bc6a98a11244a76e40', '685ec4bc6a98a11244a76e41', '685ec4bc6a98a11244a76e42', '685ec4bc6a98a11244a76e43', '685ec4bc6a98a11244a76e44', '685ec4bc6a98a11244a76e45', '685ec4bc6a98a11244a76e46', '685ec4bc6a98a11244a76e47', '685ec4bc6a98a11244a76e48', '685ec4bc6a98a11244a76e49', '685ec4bc6a98a11244a76e4a', '685ec4bc6a98a11244a76e4b', '685ec4bc6a98a11244a76e4c', '685ec4bc6a98a11244a76e4d', '685ec4bc6a98a11244a76e4e', '685ec4bc6a98a11244a76e4f', '685ec4bc6a98a11244a76e50', '685ec4bc6a98a11244a76e51', '685ec4bc6a98a11244a76e52', '685ec4bc6a98a11244a76e53', '685ec4bc6a98a11244a76e54', '685ec4bc6a98a11244a76e55', '685ec4bc6a98a11244a76e56', '685ec4bc6a98a11244a76e57', '685ec4bc6a98a11244a76e58', '685ec4bc6a98a11244a76e59', '685ec4bc6a98a11244a76e5a', '685ec4bc6a98a11244a76e5b', '685ec4bc6a98a11244a76e5c', '685ec4bc6a98a11244a76e5d', '685ec4bc6a98a11244a76e5e', '685ec4bc6a98a11244a76e5f', '685ec4bc6a98a11244a76e60', '685ec4bc6a98a11244a76e61', '685ec4bc6a98a11244a76e62', '685ec4bc6a98a11244a76e63', '685ec4bc6a98a11244a76e64', '685ec4bc6a98a11244a76e65', '685ec4bc6a98a11244a76e66', '685ec4bc6a98a11244a76e67', '685ec4bc6a98a11244a76e68', '685ec4bc6a98a11244a76e69', '685ec4bc6a98a11244a76e6a', '685ec4bc6a98a11244a76e6b', '685ec4bc6a98a11244a76e6c', '685ec4bc6a98a11244a76e6d', '685ec4bc6a98a11244a76e6e', '685ec4bc6a98a11244a76e6f', '685ec4bc6a98a11244a76e70', '685ec4bc6a98a11244a76e71', '685ec4bc6a98a11244a76e72', '685ec4bc6a98a11244a76e73', '685ec4bc6a98a11244a76e74', '685ec4bc6a98a11244a76e75', '685ec4bc6a98a11244a76e76', '685ec4bc6a98a11244a76e77', '685ec4bc6a98a11244a76e78', '685ec4bc6a98a11244a76e79', '685ec4bc6a98a11244a76e7a', '685ec4bc6a98a11244a76e7b', '685ec4bc6a98a11244a76e7c', '685ec4bc6a98a11244a76e7d', '685ec4bc6a98a11244a76e7e', '685ec4bc6a98a11244a76e7f', '685ec4bc6a98a11244a76e80', '685ec4bc6a98a11244a76e81', '685ec4bc6a98a11244a76e82', '685ec4bc6a98a11244a76e83', '685ec4bc6a98a11244a76e84', '685ec4bc6a98a11244a76e85', '685ec4bc6a98a11244a76e86', '685ec4bc6a98a11244a76e87', '685ec4bc6a98a11244a76e88', '685ec4bc6a98a11244a76e89', '685ec4bc6a98a11244a76e8a', '685ec4bc6a98a11244a76e8b', '685ec4bc6a98a11244a76e8c', '685ec4bc6a98a11244a76e8d', '685ec4bc6a98a11244a76e8e', '685ec4bc6a98a11244a76e8f', '685ec4bc6a98a11244a76e90', '685ec4bc6a98a11244a76e91', '685ec4bc6a98a11244a76e92', '685ec4bc6a98a11244a76e93', '685ec4bc6a98a11244a76e94', '685ec4bc6a98a11244a76e95', '685ec4bc6a98a11244a76e96', '685ec4bc6a98a11244a76e97', '685ec4bc6a98a11244a76e98', '685ec4bc6a98a11244a76e99', '685ec4bc6a98a11244a76e9a', '685ec4bc6a98a11244a76e9b', '685ec4bc6a98a11244a76e9c', '685ec4bc6a98a11244a76e9d', '685ec4bc6a98a11244a76e9e', '685ec4bc6a98a11244a76e9f', '685ec4bc6a98a11244a76ea0', '685ec4bc6a98a11244a76ea1', '685ec4bc6a98a11244a76ea2', '685ec4bc6a98a11244a76ea3', '685ec4bc6a98a11244a76ea4', '685ec4bc6a98a11244a76ea5', '685ec4bc6a98a11244a76ea6', '685ec4bc6a98a11244a76ea7', '685ec4bc6a98a11244a76ea8', '685ec4bc6a98a11244a76ea9', '685ec4bc6a98a11244a76eaa', '685ec4bc6a98a11244a76eab', '685ec4bc6a98a11244a76eac', '685ec4bc6a98a11244a76ead', '685ec4bc6a98a11244a76eae', '685ec4bc6a98a11244a76eaf', '685ec4bc6a98a11244a76eb0', '685ec4bc6a98a11244a76eb1', '685ec4bc6a98a11244a76eb2', '685ec4bc6a98a11244a76eb3', '685ec4bc6a98a11244a76eb4', '685ec4bc6a98a11244a76eb5', '685ec4bc6a98a11244a76eb6', '685ec4bc6a98a11244a76eb7', '685ec4bc6a98a11244a76eb8', '685ec4bc6a98a11244a76eb9', '685ec4bc6a98a11244a76eba', '685ec4bc6a98a11244a76ebb', '685ec4bc6a98a11244a76ebc', '685ec4bc6a98a11244a76ebd', '685ec4bc6a98a11244a76ebe', '685ec4bc6a98a11244a76ebf', '685ec4bc6a98a11244a76ec0', '685ec4bc6a98a11244a76ec1', '685ec4bc6a98a11244a76ec2', '685ec4bc6a98a11244a76ec3', '685ec4bc6a98a11244a76ec4', '685ec4bc6a98a11244a76ec5', '685ec4bc6a98a11244a76ec6', '685ec4bc6a98a11244a76ec7', '685ec4bc6a98a11244a76ec8', '685ec4bc6a98a11244a76ec9', '685ec4bc6a98a11244a76eca', '685ec4bc6a98a11244a76ecb', '685ec4bc6a98a11244a76ecc', '685ec4bc6a98a11244a76ecd', '685ec4bc6a98a11244a76ece', '685ec4bc6a98a11244a76ecf', '685ec4bc6a98a11244a76ed0', '685ec4bc6a98a11244a76ed1', '685ec4bc6a98a11244a76ed2', '685ec4bc6a98a11244a76ed3', '685ec4bc6a98a11244a76ed4', '685ec4bc6a98a11244a76ed5', '685ec4bc6a98a11244a76ed6', '685ec4bc6a98a11244a76ed7', '685ec4bc6a98a11244a76ed8', '685ec4bc6a98a11244a76ed9', '685ec4bc6a98a11244a76eda', '685ec4bc6a98a11244a76edb', '685ec4bc6a98a11244a76edc', '685ec4bc6a98a11244a76edd', '685ec4bc6a98a11244a76ede', '685ec4bc6a98a11244a76edf', '685ec4bc6a98a11244a76ee0', '685ec4bc6a98a11244a76ee1', '685ec4bc6a98a11244a76ee2', '685ec4bc6a98a11244a76ee3', '685ec4bc6a98a11244a76ee4', '685ec4bc6a98a11244a76ee5', '685ec4bc6a98a11244a76ee6', '685ec4bc6a98a11244a76ee7', '685ec4bc6a98a11244a76ee8', '685ec4bc6a98a11244a76ee9', '685ec4bc6a98a11244a76eea', '685ec4bc6a98a11244a76eeb', '685ec4bc6a98a11244a76eec', '685ec4bc6a98a11244a76eed', '685ec4bc6a98a11244a76eee', '685ec4bc6a98a11244a76eef', '685ec4bc6a98a11244a76ef0', '685ec4bc6a98a11244a76ef1', '685ec4bc6a98a11244a76ef2', '685ec4bc6a98a11244a76ef3', '685ec4bc6a98a11244a76ef4', '685ec4bc6a98a11244a76ef5', '685ec4bc6a98a11244a76ef6', '685ec4bc6a98a11244a76ef7', '685ec4bc6a98a11244a76ef8', '685ec4bc6a98a11244a76ef9', '685ec4bc6a98a11244a76efa', '685ec4bc6a98a11244a76efb', '685ec4bc6a98a11244a76efc', '685ec4bc6a98a11244a76efd', '685ec4bc6a98a11244a76efe', '685ec4bc6a98a11244a76eff', '685ec4bc6a98a11244a76f00', '685ec4bc6a98a11244a76f01', '685ec4bc6a98a11244a76f02', '685ec4bc6a98a11244a76f03', '685ec4bc6a98a11244a76f04', '685ec4bc6a98a11244a76f05', '685ec4bc6a98a11244a76f06', '685ec4bc6a98a11244a76f07', '685ec4bc6a98a11244a76f08', '685ec4bc6a98a11244a76f09', '685ec4bc6a98a11244a76f0a', '685ec4bc6a98a11244a76f0b', '685ec4bc6a98a11244a76f0c', '685ec4bc6a98a11244a76f0d', '685ec4bc6a98a11244a76f0e', '685ec4bc6a98a11244a76f0f', '685ec4bc6a98a11244a76f10', '685ec4bc6a98a11244a76f11', '685ec4bc6a98a11244a76f12', '685ec4bc6a98a11244a76f13', '685ec4bc6a98a11244a76f14', '685ec4bc6a98a11244a76f15', '685ec4bc6a98a11244a76f16', '685ec4bc6a98a11244a76f17', '685ec4bc6a98a11244a76f18', '685ec4bc6a98a11244a76f19', '685ec4bc6a98a11244a76f1a', '685ec4bc6a98a11244a76f1b', '685ec4bc6a98a11244a76f1c', '685ec4bc6a98a11244a76f1d', '685ec4bc6a98a11244a76f1e', '685ec4bc6a98a11244a76f1f', '685ec4bc6a98a11244a76f20', '685ec4bc6a98a11244a76f21', '685ec4bc6a98a11244a76f22', '685ec4bc6a98a11244a76f23', '685ec4bc6a98a11244a76f24', '685ec4bc6a98a11244a76f25', '685ec4bc6a98a11244a76f26', '685ec4bc6a98a11244a76f27', '685ec4bc6a98a11244a76f28', '685ec4bc6a98a11244a76f29', '685ec4bc6a98a11244a76f2a', '685ec4bc6a98a11244a76f2b', '685ec4bc6a98a11244a76f2c', '685ec4bc6a98a11244a76f2d', '685ec4bc6a98a11244a76f2e', '685ec4bc6a98a11244a76f2f', '685ec4bc6a98a11244a76f30', '685ec4bc6a98a11244a76f31', '685ec4bc6a98a11244a76f32', '685ec4bc6a98a11244a76f33', '685ec4bc6a98a11244a76f34', '685ec4bc6a98a11244a76f35', '685ec4bc6a98a11244a76f36', '685ec4bc6a98a11244a76f37', '685ec4bc6a98a11244a76f38', '685ec4bc6a98a11244a76f39', '685ec4bc6a98a11244a76f3a', '685ec4bc6a98a11244a76f3b', '685ec4bc6a98a11244a76f3c', '685ec4bc6a98a11244a76f3d', '685ec4bc6a98a11244a76f3e', '685ec4bc6a98a11244a76f3f', '685ec4bc6a98a11244a76f40', '685ec4bc6a98a11244a76f41', '685ec4bc6a98a11244a76f42', '685ec4bc6a98a11244a76f43', '685ec4bc6a98a11244a76f44', '685ec4bc6a98a11244a76f45', '685ec4bc6a98a11244a76f46', '685ec4bc6a98a11244a76f47', '685ec4bc6a98a11244a76f48', '685ec4bc6a98a11244a76f49', '685ec4bc6a98a11244a76f4a', '685ec4bc6a98a11244a76f4b', '685ec4bc6a98a11244a76f4c', '685ec4bc6a98a11244a76f4d', '685ec4bc6a98a11244a76f4e', '685ec4bc6a98a11244a76f4f', '685ec4bc6a98a11244a76f50', '685ec4bc6a98a11244a76f51', '685ec4bc6a98a11244a76f52', '685ec4bc6a98a11244a76f53', '685ec4bc6a98a11244a76f54', '685ec4bc6a98a11244a76f55', '685ec4bc6a98a11244a76f56', '685ec4bc6a98a11244a76f57', '685ec4bc6a98a11244a76f58', '685ec4bc6a98a11244a76f59', '685ec4bc6a98a11244a76f5a', '685ec4bc6a98a11244a76f5b', '685ec4bc6a98a11244a76f5c', '685ec4bc6a98a11244a76f5d', '685ec4bc6a98a11244a76f5e', '685ec4bc6a98a11244a76f5f']\n",
            "Document chunks added to MongoDB vector database.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0c7658d"
      },
      "source": [
        "from typing import TypedDict, List\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class ChatGraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of the LangGraph for the chat workflow.\n",
        "\n",
        "    Attributes:\n",
        "        chat_history: A list of BaseMessage objects representing the conversation history.\n",
        "        user_query: The current user's query string.\n",
        "        retrieved_documents: A list of Document objects retrieved from the vector database.\n",
        "    \"\"\"\n",
        "    chat_history: List[BaseMessage]\n",
        "    user_query: str\n",
        "    retrieved_documents: List[Document]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ad46595"
      },
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "from urllib.parse import urlparse\n",
        "from langchain_mongodb.retrievers import MongoDBAtlasHybridSearchRetriever\n",
        "\n",
        "def retrieve_node(state: ChatGraphState) -> ChatGraphState:\n",
        "    \"\"\"\n",
        "    Processes user input and retrieves relevant documents using hybrid search and multi-query retrieval.\n",
        "    \"\"\"\n",
        "    print(\"---RETRIEVING DOCUMENTS (Hybrid Search)---\")\n",
        "    user_query = state['user_query']\n",
        "    chat_history = state['chat_history']\n",
        "\n",
        "    if 'collection' not in globals() or 'embeddings' not in globals() or 'llm' not in globals():\n",
        "         raise ValueError(\"Retrieval dependencies (collection, embeddings, llm) not initialized.\")\n",
        "\n",
        "    vectorstore_instance = MongoDBAtlasVectorSearch(\n",
        "        collection=collection, embedding=embeddings, index_name=\"vector_index\"\n",
        "    )\n",
        "    print(\"MongoDBAtlasVectorSearch initialized for hybrid retrieval.\")\n",
        "\n",
        "\n",
        "    if 'repo_url' not in globals():\n",
        "         print(\"Warning: repo_url not found globally. Hybrid search pre-filter will not be applied.\")\n",
        "         repo_name_filter = None\n",
        "    else:\n",
        "         try:\n",
        "            parsed_url = urlparse(repo_url)\n",
        "            path_parts = parsed_url.path.strip('/').split('/')\n",
        "            if len(path_parts) >= 2:\n",
        "                repo_name = path_parts[1]\n",
        "                repo_name_filter = {\"file_path\": {\"$regex\": f\"^{repo_name}/.*|^\\\\.gitignore$|^LICENSE$\"}}\n",
        "            else:\n",
        "                print(f\"Warning: Could not extract repo_name from repo_url: {repo_url}. Hybrid search pre-filter will not be applied.\")\n",
        "                repo_name_filter = None\n",
        "         except Exception as e:\n",
        "             print(f\"Warning: Error extracting repo_name from repo_url: {e}. Hybrid search pre-filter will not be applied.\")\n",
        "             repo_name_filter = None\n",
        "\n",
        "\n",
        "    hybrid_retriever = MongoDBAtlasHybridSearchRetriever(\n",
        "        vectorstore=vectorstore_instance, # Pass the vectorstore instance\n",
        "        search_index_name=\"text_search_index\", # Assuming this is your text search index name\n",
        "        # pre_filter=repo_name_filter, # Apply pre-filter if repo_name was successfully extracted\n",
        "        top_k=4, # Number of documents to retrieve\n",
        "    )\n",
        "\n",
        "    # Initialize the Multi-Query Retriever\n",
        "    retriever_from_llm = MultiQueryRetriever.from_llm(\n",
        "        retriever=hybrid_retriever, llm=llm\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Invoke the Multi-Query Retriever\n",
        "        retrieved_documents = retriever_from_llm.invoke(user_query)\n",
        "        print(f\"Retrieved {len(retrieved_documents)} relevant documents using hybrid search.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during hybrid document retrieval: {e}\")\n",
        "        retrieved_documents = []\n",
        "\n",
        "\n",
        "    # Append the user query to the chat history\n",
        "    chat_history.append(HumanMessage(content=user_query))\n",
        "\n",
        "    return {**state, 'retrieved_documents': retrieved_documents, 'chat_history': chat_history}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3726f9bd"
      },
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "def generate_response_node(state: ChatGraphState) -> ChatGraphState:\n",
        "    \"\"\"\n",
        "    Generates a response using the LLM based on the user query and retrieved documents.\n",
        "    \"\"\"\n",
        "    print(\"---GENERATING RESPONSE---\")\n",
        "    user_query = state['user_query']\n",
        "    chat_history = state['chat_history']\n",
        "    retrieved_documents = state['retrieved_documents']\n",
        "\n",
        "    # Ensure llm is accessible in this scope (assuming it's a global variable or passed in)\n",
        "    if 'llm' not in globals():\n",
        "         raise ValueError(\"LLM is not initialized. Please ensure the 'llm' variable is set.\")\n",
        "\n",
        "\n",
        "    # Create a prompt template\n",
        "    # We include chat history for context in the conversation\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"You are a helpful AI assistant that answers questions about GitHub repository security vulnerabilities based on the provided context.\"),\n",
        "        (\"human\", \"Here is the relevant context:\\n{context}\"),\n",
        "        (\"human\", \"Here is the conversation history:\\n{chat_history}\"),\n",
        "        (\"human\", \"{query}\")\n",
        "    ])\n",
        "\n",
        "    # Format the retrieved documents into a single string for the prompt context\n",
        "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in retrieved_documents])\n",
        "\n",
        "    # Create the LLM chain\n",
        "    # We use RunnablePassthrough to pass the context and query to the prompt\n",
        "    rag_chain = (\n",
        "        RunnablePassthrough.assign(context=lambda x: context_text)\n",
        "        | prompt\n",
        "        | llm\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Invoke the LLM chain\n",
        "        response = rag_chain.invoke({\"query\": user_query, \"chat_history\": chat_history})\n",
        "        # The response object's content contains the generated text\n",
        "        ai_response_content = response.content\n",
        "\n",
        "    except Exception as e:\n",
        "        ai_response_content = f\"Error generating response: {e}\"\n",
        "        print(ai_response_content)\n",
        "\n",
        "\n",
        "    # Append the AI response to the chat history\n",
        "    chat_history.append(AIMessage(content=ai_response_content))\n",
        "\n",
        "    print(\"Response generated.\")\n",
        "\n",
        "    return {**state, 'chat_history': chat_history}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edb2e32f",
        "outputId": "4a5faaee-3c00-41d1-97a5-5675ad0431ae"
      },
      "source": [
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# Instantiate the StateGraph with the defined ChatGraphState\n",
        "chat_workflow = StateGraph(ChatGraphState)\n",
        "\n",
        "# Add nodes to the graph\n",
        "chat_workflow.add_node(\"retrieve\", retrieve_node)\n",
        "chat_workflow.add_node(\"generate_response\", generate_response_node)\n",
        "\n",
        "# Set the entry point\n",
        "# The workflow will start with the retrieval node when a user query is provided\n",
        "chat_workflow.set_entry_point(\"retrieve\")\n",
        "\n",
        "# Add an edge from retrieve to generate_response\n",
        "# After retrieving documents, the workflow should move to generating a response\n",
        "chat_workflow.add_edge(\"retrieve\", \"generate_response\")\n",
        "\n",
        "# Set the finish point\n",
        "# The workflow finishes after generating a response\n",
        "chat_workflow.set_finish_point(\"generate_response\")\n",
        "\n",
        "# Compile the graph\n",
        "chat_app = chat_workflow.compile()\n",
        "\n",
        "print(\"LangGraph chat workflow defined and compiled.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LangGraph chat workflow defined and compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "60b8681d",
        "outputId": "5ef70f28-1e34-4d39-dfa9-e6557743748d"
      },
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Define the initial state for the chat workflow\n",
        "# The chat_history should be a list of BaseMessage objects\n",
        "initial_chat_state = {\n",
        "    \"chat_history\": [],\n",
        "    \"user_query\": \"\", # Placeholder for the current user query\n",
        "    \"retrieved_documents\": [] # Placeholder for retrieved documents\n",
        "}\n",
        "\n",
        "# Simple interaction loop\n",
        "print(\"Chatbot initialized. Ask me about the repository's security vulnerabilities! Type 'quit' to exit.\")\n",
        "\n",
        "# Keep track of the current state to maintain chat history\n",
        "current_chat_state = initial_chat_state.copy()\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "    if user_input.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update the current state with the new user query\n",
        "    current_chat_state['user_query'] = user_input\n",
        "\n",
        "    try:\n",
        "        # Invoke the compiled LangGraph application\n",
        "        # We pass the current state to maintain chat history and use the new user query\n",
        "        final_chat_state = chat_app.invoke(current_chat_state)\n",
        "\n",
        "        # The generate_response_node updates the chat_history with the AI's response\n",
        "        # The last message in the history will be the latest AI response\n",
        "        latest_ai_message = final_chat_state['chat_history'][-1]\n",
        "        print(f\"Bot: {latest_ai_message.content}\")\n",
        "\n",
        "        # Update the current_chat_state for the next turn, excluding the user_query\n",
        "        # as it's specific to the last turn. The chat_history is already updated in the node.\n",
        "        current_chat_state = {\n",
        "            \"chat_history\": final_chat_state['chat_history'],\n",
        "            \"user_query\": \"\", # Reset user_query for the next turn\n",
        "            \"retrieved_documents\": [] # Reset retrieved_documents for the next turn\n",
        "        }\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during processing: {e}\")\n",
        "        # Optionally, you could add a message to the chat history indicating the error\n",
        "        # current_chat_state['chat_history'].append(AIMessage(content=\"Sorry, an error occurred.\"))\n",
        "\n",
        "\n",
        "print(\"Chatbot session ended.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot initialized. Ask me about the repository's security vulnerabilities! Type 'quit' to exit.\n",
            "You: what is name of the project?\n",
            "---RETRIEVING DOCUMENTS (Hybrid Search)---\n",
            "MongoDBAtlasVectorSearch initialized for hybrid retrieval.\n",
            "Retrieved 12 relevant documents using hybrid search.\n",
            "---GENERATING RESPONSE---\n",
            "Response generated.\n",
            "Bot: Based on the provided context, the name of the application/project is **Gastro**.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-1592655773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}